'''
adapted from trl/examples/research_projects/stack_llama_2/scripts/dpo_llama2.py

Due to data processing steps (data format and model template), this script only supports training LLaMA-1 on the HH-RLHF dataset and training Alpaca on the PKU-SafeRLHF dataset.
For other dataset and models, we need to modify the data processing steps.
'''
import os
from dataclasses import dataclass, field
from typing import Dict, Optional
import torch
from accelerate import Accelerator
from datasets import Dataset, load_dataset
from peft import PBLoraConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
from trl import DPOTrainer, DPOConfig
from trl.commands.cli_utils import TrlParser

from pref_arm_trainer import PrefARMTrainer

import wandb
#wandb.init(mode="disabled")

@dataclass
class ARMConfig(DPOConfig):
    gamma: Optional[float] = field(
        default=0.0,
        metadata={"help": "target reward margin gamma. The reward margin is reward_win - reward_lose - gamma, where \
                                                          reward_win (reward_lose) = beta * log pi_arm_win (lose)."},
    )
    length_normalization: Optional[bool] = field(
        default=False,
        metadata={"help": "Whether to normalize the logprobs using the length of the response. length_normalization=True is not default for ARM and should only be used for testing purposes!"},
    )

@dataclass
class ScriptArguments:
    """
    The arguments for the DPO/ARM training script.

    NOTE: other training arguments, such as learning rate and beta, should be set in the command line.
    They are included in DPOConfig, not here. ScriptArguments below are arguments that are not included in DPOConfig.
    """
    # used by DPO or ARM
    # algorithm: Optional[str] = field(default="arm", metadata={"help": "algorithm to use [dpo, arm]"})
    model_name_or_path: Optional[str] = field(
        default="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        metadata={"help": "the location of the to-be-finetuned model name or path"},
    )

    # dataset
    preference_dataset: Optional[str] = field(
        default="hh_rlhf", metadata={"help": ""},
    )

    # training
    optimizer_type: Optional[str] = field(default="paged_adamw_32bit", metadata={"help": "the optimizer type"})

    # model
    lora_alpha: Optional[float] = field(default=16, metadata={"help": "the lora alpha parameter"})
    lora_dropout: Optional[float] = field(default=0.05, metadata={"help": "the lora dropout parameter"})
    lora_r: Optional[int] = field(default=8, metadata={"help": "the lora r parameter"})
    lora_r2: Optional[int] = field(default=8, metadata={"help": "for mixlora only"})

    load_in_4bit: Optional[bool] = field(default=True, metadata={"help": "whether to load the model in 4bit"})
    model_dtype: Optional[str] = field(
        default="float16", metadata={"help": "model_dtype[float16, bfloat16, float] for loading."}
    )

    # others
    sanity_check: Optional[bool] = field(default=False, metadata={"help": "only train on 1000 samples"})
    ignore_bias_buffers: Optional[bool] = field(
        default=False,
        metadata={
            "help": "fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See"
            "https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992"
        },
    )
    # tasks
    safe_obj: Optional[bool] = field(default=True, metadata={"help": ""})
    help_obj: Optional[bool] = field(default=True, metadata={"help": ""})
    humor_obj: Optional[bool] = field(default=True, metadata={"help": ""})
    # beta in loss
    beta_safe: Optional[float] = field(default=0.5, metadata={"help": ""})
    beta_help: Optional[float] = field(default=0.5, metadata={"help": ""})
    beta_humor: Optional[float] = field(default=0.5, metadata={"help": ""})
    #
    pref_sample_p: Optional[float] = field(default=1.0, metadata={"help": ""})

def _parse_dialogue(dialogue_string: str) -> list[dict]:
    """
    Anthropic ìŠ¤íƒ€ì¼ì˜ "\n\nHuman: ... \n\nAssistant: ..." ë¬¸ìì—´ì„
    Hugging Faceì˜ messages ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    messages = []
    # ë°ì´í„°ëŠ” í•­ìƒ Human: í„´ìœ¼ë¡œ ì‹œì‘í•˜ê³  Assistant: í„´ìœ¼ë¡œ ëë‚¨ (Assistant: ë’¤ëŠ” ë¹„ì–´ìˆìŒ)
    
    # ë§¨ ì•ì˜ \n\nì„ ì œê±°í•©ë‹ˆë‹¤.
    dialogue_string = dialogue_string.strip() 

    # 'Assistant:'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆ•ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ ê° í„´ì´ [Human: Q] [Assistant: A] ìŒìœ¼ë¡œ ë‚˜ë‰˜ê¸° ì‰½ìŠµë‹ˆë‹¤.
    
    # 1. ë§ˆì§€ë§‰ Assistant: íƒœê·¸ë¥¼ ì œê±°í•˜ì—¬ ì‘ë‹µ ì§ì „ê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    if dialogue_string.endswith("\n\nAssistant:"):
        dialogue_string = dialogue_string[:-len("\n\nAssistant:")].strip()
    
    # 2. ëŒ€í™” ê¸°ë¡ì„ í„´(Turn)ë³„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    # ì²« í„´ì€ Humanì´ë¯€ë¡œ, Human:ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    turns = dialogue_string.split("\n\nHuman:") 
    
    # ì²« ìš”ì†ŒëŠ” ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ ì‹œì‘ ë¶€ë¶„ì˜ ì”ì—¬ë¬¼ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°
    turns = [t.strip() for t in turns if t.strip()]

    for full_turn in turns:
        if "\n\nAssistant:" in full_turn:
            # ì¤‘ê°„ í„´: "ì§ˆë¬¸ \n\nAssistant: ë‹µë³€" í˜•íƒœ
            try:
                user_content, assistant_content = full_turn.split("\n\nAssistant:", 1)
                messages.append({"role": "user", "content": user_content.strip()})
                messages.append({"role": "assistant", "content": assistant_content.strip()})
            except ValueError:
                # ì˜ˆìƒì¹˜ ëª»í•œ ë¶„í•  (ì˜¤ë¥˜ ì²˜ë¦¬)
                print(f"Warning: Could not split turn cleanly: {full_turn[:50]}...")
                continue
        else:
            # ë§ˆì§€ë§‰ í„´: "ìµœì¢… ì§ˆë¬¸" í˜•íƒœ (ë§ˆì§€ë§‰ Assistant: íƒœê·¸ëŠ” ì´ë¯¸ ì œê±°ë˜ì—ˆìŒ)
            messages.append({"role": "user", "content": full_turn.strip()})
    return messages

def get_hh_rlhf(
    dataset_name: str,
    sanity_check: bool = False,
    num_proc=4,
    obj_key=None,
    tokenizer=None # ğŸ‘ˆ í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìë¡œ ë°›ì•„ì•¼ apply_chat_template ì‚¬ìš© ê°€ëŠ¥
):
    train_dataset = load_dataset("json", data_files='../../data/hh/train.json', 
            split='train', num_proc=num_proc)
    test_dataset = load_dataset("json", data_files='../../data/hh/dev.json', 
            split='train', num_proc=num_proc)
    original_columns = train_dataset.column_names

    if sanity_check:
        train_dataset = train_dataset.select(range(min(len(train_dataset), 1000)))
    
    # --------------------------------------------------------------------------------------------------
    # ê¸°ì¡´ PKU/Chat Template í•¨ìˆ˜ ì œê±° (ì´ë¯¸ ìœ„ì—ì„œ _parse_dialogueë¡œ ëŒ€ì²´ë˜ì—ˆìŒ)
    # --------------------------------------------------------------------------------------------------

    def return_prompt_and_responses(sample, obj_key, tokenizer) -> Dict[str, str]:
        
        labels = {'safe': sample['safer_response_id'], 
                  "help": sample['better_response_id'],
                  "humor": sample['humorer_responses_id'],}
        
        # 1. ë©€í‹°í„´ ëŒ€í™” ê¸°ë¡ì„ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±
        messages = _parse_dialogue(sample["prompt"])
        SYSTEM_PROMPT = "You are a helpful, harmless, and humorous assistant."
        messages.insert(0, {"role": "system", "content": SYSTEM_PROMPT})

        # 2. Chosen/Rejected ì‘ë‹µ í¬ë§·íŒ…
        # chosen/rejected ì‘ë‹µì€ 'messages' ë¦¬ìŠ¤íŠ¸ì— ë§ˆì§€ë§‰ í„´ì˜ ì‘ë‹µìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.
        
        # A. í”„ë¡¬í”„íŠ¸ (Context + Final Instruction) ìƒì„±
        # add_generation_prompt=TrueëŠ” ë§ˆì§€ë§‰ <|assistant|> íƒœê·¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        # chosen/rejected ì‘ë‹µì„ ì œì™¸í•œ ëŒ€í™” ê¸°ë¡ë§Œ í…œí”Œë¦¿ì— ì ìš©í•©ë‹ˆë‹¤.
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True # <|assistant|> íƒœê·¸ë¥¼ ëì— ì¶”ê°€
        )

        # B. Chosen ì‘ë‹µê³¼ Rejected ì‘ë‹µì— í…œí”Œë¦¿ ì ìš©
        # LLaMA-2/TinyLLaMA ê³„ì—´ì€ ì‘ë‹µ ëì— </s>ë¥¼ ë¶™ì…ë‹ˆë‹¤.
        # chosen/rejectedëŠ” *ì´ë¯¸* ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ ì‘ë‹µì´ë¯€ë¡œ,
        # í…œí”Œë¦¿ ëŒ€ì‹ , </s> í† í°ë§Œ ë¶™ì—¬ì£¼ê±°ë‚˜ (í˜¹ì€ ë¶™ì´ì§€ ì•Šê±°ë‚˜, DPO íŠ¸ë ˆì´ë„ˆì— ë”°ë¼ ë‹¤ë¦„),
        # ì•ˆì „í•˜ê²ŒëŠ” ëª¨ë¸ì´ í•™ìŠµëœ ë°©ì‹ëŒ€ë¡œ í„´ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” Chosen/Rejected í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê³ , íŠ¸ë ˆì´ë„ˆê°€ ë‚˜ë¨¸ì§€ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        
        # *Note*: TRLì˜ DPO/ARM íŠ¸ë ˆì´ë„ˆëŠ” ë³´í†µ promptì™€ chosen/rejected í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•©ë‹ˆë‹¤.
        
        return {
            "prompt": formatted_prompt,
            "chosen": sample["response_0"],
            "rejected": sample["response_1"],
            "labels": {obj: labels[obj] for obj in obj_key}
        }
        
    return_prompt_and_responses_with_version = lambda x: return_prompt_and_responses(x, obj_key, tokenizer)

    # need to set batched=False because return_prompt_and_responses_with_version can only be applied to a single sample
    return train_dataset.map(
        return_prompt_and_responses_with_version,
        batched=False, 
        num_proc=num_proc,
        remove_columns=original_columns,
    ), test_dataset.map(
        return_prompt_and_responses_with_version,
        batched=False,
        num_proc=num_proc,
        remove_columns=original_columns,
    )
    ##### chat template ends #####

if __name__ == "__main__":

    parser = TrlParser((ScriptArguments, ARMConfig))
    script_args, training_args = parser.parse_args_and_config()

    print(f'\nPreference dataset: {script_args.preference_dataset}\n')

    training_args.gradient_checkpointing_kwargs={"use_reentrant": False} # this is necessary due to https://github.com/huggingface/trl/issues/480

    # assert script_args.algorithm in ["dpo", "arm"], "algorithm must be either dpo or arm"
    if training_args.gamma > 0:
        assert training_args.loss_type == "sigmoid", "gamma (the target_reward_margin) is only supported for sigmoid loss."

    set_seed(training_args.seed)

    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path)
    tokenizer.pad_token = tokenizer.eos_token

    # 1. Load the preference dataset
    if script_args.preference_dataset in ["hh_rlhf"]:
        training_args.obj_key, training_args.beta_obj = [], []
        if script_args.safe_obj:
            training_args.obj_key.append('safe')
            training_args.beta_obj.append(script_args.beta_safe)
        if script_args.help_obj:
            training_args.obj_key.append('help')
            training_args.beta_obj.append(script_args.beta_help)
        if script_args.humor_obj:
            training_args.obj_key.append('humor')
            training_args.beta_obj.append(script_args.beta_humor)
        train_dataset, eval_dataset = get_hh_rlhf(dataset_name=script_args.preference_dataset, 
                                                  sanity_check=script_args.sanity_check, 
                                                  obj_key=training_args.obj_key,
                                                  tokenizer=tokenizer)
    else:
        raise ValueError(f"Invalid preference dataset: {script_args.preference_dataset}")

    print(f'\nBefore filtering. Train data size: {train_dataset.num_rows}, Test data size: {eval_dataset.num_rows}\n')

    train_dataset = train_dataset.filter(
        lambda x: len(x["prompt"]) + len(x["chosen"]) <= training_args.max_length
        and len(x["prompt"]) + len(x["rejected"]) <= training_args.max_length
    )
    eval_dataset = eval_dataset.filter(
        lambda x: len(x["prompt"]) + len(x["chosen"]) <= training_args.max_length
        and len(x["prompt"]) + len(x["rejected"]) <= training_args.max_length
    )
    print(f'After filtering. Train data size: {train_dataset.num_rows}, Test data size: {eval_dataset.num_rows}\n')

    # 2. load a pretrained model
    torch_dtype = torch.float
    if script_args.model_dtype == "float16":
        torch_dtype = torch.float16
    elif script_args.model_dtype == "bfloat16":
        torch_dtype = torch.bfloat16

    print(f'\n model_name_or_path: {script_args.model_name_or_path}\n')
    model = AutoModelForCausalLM.from_pretrained(
        script_args.model_name_or_path,
        low_cpu_mem_usage=True,
        torch_dtype=torch_dtype,
        load_in_4bit=script_args.load_in_4bit,
        device_map={"": Accelerator().local_process_index},
    )
    model.config.use_cache = False

    if script_args.ignore_bias_buffers:
        # torch distributed hack
        model._ddp_params_and_buffers_to_ignore = [
            name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool
        ]

    # 3. initialize training arguments (done in DPOconfig) and peft config
    peft_config = PBLoraConfig(
        r1=script_args.lora_r,
        r2=script_args.lora_r2,
        obj_num=len(training_args.obj_key),
        lora_alpha=script_args.lora_alpha,
        lora_dropout=script_args.lora_dropout,
        target_modules=[
            "q_proj",
            "v_proj",
            "k_proj",
        ],
        bias="none",
        task_type="CAUSAL_LM",
    )

    # 4. initialize the DPO/ARM trainer
    print('\n**********************************')
    training_args.pref_sample_p = script_args.pref_sample_p
    trainer = PrefARMTrainer(
        model=model,
        ref_model=None, # if not using peft, pass the model to bypass DPO Trainer check for ref model but is NOT actually used; if using peft (which is the case here), just pass None
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        peft_config=peft_config,
    )

    # 5. train
    trainer.train()
    trainer.save_model(training_args.output_dir)

    # 6. save
    output_dir = os.path.join(training_args.output_dir, "final_checkpoint")
    trainer.model.save_pretrained(output_dir)
